---
title: "Inverse Propensity Weighting (IPW)"
author: "A. Keil and M.Kamenetsky"
date: "`r Sys.Date()`"
output:
  html_document:
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,cache=TRUE)
```


```{r}
#Load in libraries
library(ggplot2) #for plotting
library(dplyr) #for data cleaning
library(survival) #for survival analysis
library(boot) #for bootstrapping
library(Hmisc) #for restricted cubic splines
library(zoo) #for handling last observation carried forward
```



   
## Research Questions:

- 1) What is the association between cumulative exposure and mortality? 
- 2) What is the impact of a personal exposure limit on mortality?
- 3) What is the effect of any feasible personal exposure limit on mortality?


## Learning Objectives:

By the end of this tutorial, you will be able to:

- Perform inverse probability weighting for longitudinal data
- Obtain bootstrapped confidence intervals for risk differences and log risk ratios across different treatment regimes


## IPW (briefly)

The class of **G-methods** includes g-estimation, g-formula/g-computation, and inverse probability weighting (IPW). IPW has several advantages over the other methods including:

- is simpler and less computationally demanding
- fewer modeling assumption
- easier to interpret

We can conceptualize IPW estimates as the results of an experiment. By weighting observational data, we create a *pseudo-population*. Each pseudo-population represents an arm of the experiment under causal assumptions. Each arm of the experiment is a *time-fixed* exposure, thereby eliminating time-varying confounding.


Strengths:

- addresses a key bias
- simpler than other g-methods
- weaker modeling assumptions
- many published examples
- focuses effect estimates on individual health

Limitations:

- Variance may be high
- sparse data/non-positivity will be problematic
- unfamiliar approach relative to more commonly-used mortality models with cumulative exposure


## Approach to Weighting

Standard inverse probability weighting is used for studying cumulative exposure-response curves. However we can use IPW to study the effects of hypothetical exposure limits. For example, we may want to know if person A is at work and their annual exposure can be no more than 2.0 units, what do we expect their response to be? This is sometimes called a **clone-censor-weight** approach.

## Clone-Censor-Weight Approach


The idea is to first, **clone** the cohort data for each hypothetical *exposure limit*. Next, we **censor** the clones on when they exceed that limit. Finally, we estimate **weights** based on the probability of *not* being censored. The weighted data approximates the cohort had the exposure limit been placed.

From the observed cohort data, we create two clones. Clone 1 experiences no censoring, clones 2 experiences censoring when the exposure is greater or equal to the 2.0 limit we are interested in. Clone 1 (uncensored) is not weighted, whereas clone 2 is weighted when the exposure is less than the 2.0 limit. The weighted model (for example, a weighted Cox model) is used to combine data with the exposed variable as the only predictor. 


The weight is the time-specific, inverse, cumulative probability of **not** begin censored and is set to 0 when censoring occurs. The intuition is that as individuals are censored, the weights of uncensored individuals increase to represent those individuals had they remained below the limit.

## Checking Assumptions

We want to ensure that the positivity assumption (that there is a non-zero probability that every individual receives any given level of the exposures) holds. We can do this by comparing a density plot of the confounder for the whole cohort to the uncensored at the limit.

## Comparison

The comparison method under the "natural course" regime is to fit a Cox model and compare the hazard under the natural course to the hazard under the intervention.  In the Cox model, it is important to specify that errors should be clustered on person id. This is necessary to get a robust variance in order to account for the fact that individuals appear in the data multiple times. An alternative to clustering the errors, bootstrapping could be used as well. 

<!-- ## Workers Cohort -->

<!-- Consider a (simulated) cohort of 10,000 workers, where we are interested in 2 outcomes: 1) lung cancer (*D1*) and 2) other cause-specific mortality (*D2*) based on US mortality rates (race/sex/age/year-specific). -->


<!-- There is one occupational exposure of interest (*X*) for chronic effects on lung cancer outcome *D1*. Covariates of interest include race, sex, age, year, wage status (salary vs. wage at hire), and employment status ($L_t$). -->


<!-- ```{r} -->
<!-- library(ggdag) -->
<!-- exdag <- ggdag::dagify(L_t ~U, -->
<!--                          D1_t~U, -->
<!--                           D2_t ~ U, -->
<!--                        L_t ~ X_t_1 , -->
<!--                        X_t~ L_t, -->
<!--                        D1_t ~ X_t, -->
<!--                        D2_t ~ X_t, -->
<!--                        D1_t ~ X_t_1, -->
<!--                        D2_t ~ X_t_1) -->

<!-- #set.seed(2) -->
<!-- set.seed(18) -->
<!-- ggdag::ggdag(exdag)+ -->
<!--   theme_void() #+ -->
<!-- #  ggtitle("Example DAG") -->
<!-- ``` -->


# Uranium Mine Workers Data Example

Consider a longitudinal study where you have two data measured at two (or more) time points. 



The target population consists of uranium miners. We would like to estimate the risk (or cumulative incidence) from 0-20 years. We want to estimate the following regimes:

- No intervention ("natural course")
- 0.3 x 1000 pCi/L-year limit ("slightly below current standard")
- 0.1 x 1000 pCi/L-year limit ("well below current standard")
- No exposure ("attributable risk")

These are considered to be *dynamic regimes*, in that at work, exposure will be below the limit and unexposed otherwise.



We load in the data set `miners` using `read.csv()` function. We pipe (`%>%`) that data to the `mutate()` function to create a new variable (`lograd`), which we will use later. This variable takes the natural logarithm of the radon values when participants were at work (`atwork==1`) and is missing otherwise, as stated in the `ifelse()` function. We briefly explore a summary of the data:

```{r, class.source = 'fold-show'}
miners <- read.csv("data/miners.csv") %>%
  mutate(lograd = ifelse(atwork==1, log(rad), NA))
str(miners)

```


By using the `str()` command, we have the following 13 variables:

- `id`: a unique identifier for each individual 
- `intime`: time during which participant is at work
- `outtime`: time during which participant is not at work
- `dead`: binary indicator for dead (1) or not dead (0)
- `rad`: radon measurement
- `rad_lag1`: radon measurement lagged by 1 time period
- `cum_rad`: cumulative radon exposure
- `cum_rad_lag1`: cumulative radon exposure lagged by 1 time period
- `atwork`: binary indicator of if exposure took place at work (1) or not at work (0)
- `leavework`: binary indicator of if exposure was on leave from work (1) or not on leave from work (0) **TODO**
- `durwork`: binary indicator of if exposure occurred during work (1) or not (0)
- `durwork_lag1`: binary indicator of if exposure occurred during work (1) or not (0) lagged by 1 time period
- `smoker`: binary indicator of individual was a smoker (1) or not a smoker (0)
- `lograd`: natural logarithm of the radon measurement when at work


```{r}
summary(miners[, 2:13])


```


# Clone-Censor-Weight

There are three regimes we want to consider: 1) 0.1 limit, 2) 0.3 limit, and 3) natural course. First, we set these in the object `limit` (note: natural course will correspond to `NA` limit here):

```{r}
(limits <- c(Inf, 0.3, 0.1))

```





## Clone

First, we will create a `clones` data set. This will be the original data, but with new variables `limit` and `cloneid`. The original data will be stacked and filled in later. In the first way, we show how to do this step by step. In the second way, we automate this process using the `do.call()` and `lapply()` functions.



```{r}
clones_nc <- miners %>% mutate(limit = as.numeric(limits[1]), cloneid = paste0(1, "_", id))
clones_3 <- miners %>% mutate(limit = as.numeric(limits[2]), cloneid = paste0(2, "_", id))
clones_1 <-  miners %>% mutate(limit = as.numeric(limits[3]), cloneid = paste0(3, "_", id))


clones <- bind_rows(clones_nc, clones_3, clones_1)
  
```



```{r, eval=FALSE}
# alternative (faster) code using do.call() and lapply()
clones <- do.call(rbind,
                  lapply(1:length(limits), function(x) {  
                    df =  miners                  
                    df$limit = as.numeric(limits[x])
                    df$cloneid = paste0(x, "_", df$id)
                    df                    
                  }
                  ))

```



## Censor


Next, we will artificially censor observations at the respective limit. We keep the first observation that is censored. `cloneid` is a variable that we created above, and it duplicated each person/observation in the data set based on how many times they appear. For example, participant `id==1` has `intime` at 0, 1, 2, 3, 4 (one baseline and 4 follow-up measurements). So there are 5 entries of `cloneid=1_1` for the natural course regime. Then there will be another 5 entries for participant `id==1` for regime 0.3 limit (`cloneid=2_1`), and another 5 entries for regime 0.1 limit (`cloneid=3_1`).

By cloneid (or participant-regime identifier), we generate two new variables. `cens` is the censoring variables. For each participant-regime, it takes the cumulative sum of radon exposure and gets a 1 indicator of the cumulative sum of exposure is greater than the limit or is 0 otherwise). This is given by the `cumsum(rad > limit)` argument. The `pmin()` function then assigns each participant-regime identifier to either a 1 (their cumulative sum of radon exposure at some point exceeded the limit) or 0 (they never exceeded the limit). The variable `drop` is a type of indicator variable that will take < 2 if the participant-regime identifier was never censored and will take 2 if they were ever censored. Finally, we filter and keep only participant-regimes where they were censored.


```{r}
cens_data <- clones %>%
  group_by(cloneid) %>%
  mutate(
    cens = pmin(1,cumsum(rad > limit)),
    drop = pmin(2, cumsum(cens))
  ) %>%
  group_by() %>%
  filter(
    drop < 2
  )

```

Next, we create weights that take 0/1 to use for censoring during follow up. We will modify this data set in place. By participant-regime id (`cloneid`), we create several new variables: `one` takes a 1 everywhere, `nobs` counts the number of observations by participant-regime, and `fobs` takes a 1 if it is the first occurrence for that participant-regime and is 0 otherwise. Based on this first set of created variables, we create a second set. `fu_weight` takes a 1 if the observation is not the first and the participant was at work. Then `dconf, dcense, intervened` are all set to 0. We remove the observations `one` and `nobs` once we have used them. 


```{r}
cens_data <- cens_data %>%
  group_by(cloneid) %>%
  mutate(one = 1,
         nobs = cumsum(one),
         fobs = as.numeric(nobs == 1)) %>%
  mutate(
    fu_weight = as.numeric((nobs  > 1) & (atwork == 1)),    
    dconf = 0,
    dcens = 0,
    intervened = 0,
  ) %>%
  select(-c(one, nobs)) %>%
  ungroup()

```

## Fit Censoring Models

When fitting the censoring models, we can include pre-baseline exposures. We will fit the models if there is more than a small amount of censoring as a way to avoid any convergence problems. We will do this first for the 0.1 regime, and then automate it in a loop to loop through each regime.

### 0.1 Regime


```{r}
limidx = which(cens_data$limit == 0.1) #get indices of observations where 
tempdat = cens_data[limidx,] # filter to regime 0.1
#create max cumulative radon exposure when at work
tempdat$mxl = tempdat$cum_rad_lag1 / (tempdat$durwork_lag1 + as.numeric(tempdat$outtime==1)) 
#create restricted cubic spline for baseline exposure
outkn0 = attr(rcspline.eval(filter(tempdat, fobs==1)$outtime, nk = 4), "knots")
#create restricted cubic spline for follow-up exposures at work
outkn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$outtime, nk = 4), "knots")

```


If the sum of baseline exposures for those censored is greater than 10, then we will use weights of 1 `fobs` in logistic regression models where the outcome is 0/1 censoring. We build two models: one with no explanatory variable and one with `smoker` as a predictor. In the data set `cens_data`, for rows that correspond with the 0.1 regime, we replace the stand in variable `nconf` with the prediction from the unadjusted model (`confnmod`). Similarly, we replace `dconf` with predictions from the smoker-adjusted model (`confdmod`).

If the sum of baseline exposures for those censored is less than or equal to 10, then `nconf` and `dconf` all take 0. 

```{r}
if(sum(tempdat[tempdat$fobs==1,"cens"]) > 10){
  summary(confnmod <- glm(cens ~ 1 , data = tempdat, weight=fobs, family=binomial()))
  summary(confdmod <- glm(cens ~ smoker, data = tempdat, weight=fobs, family=binomial()))
  # only get non-zero predictions if "fobs" == 1 (eligible to be "censored" at baseline)
  cens_data[limidx,"nconf"] = tempdat$fobs*as.numeric(predict(confnmod, type="response"))
  cens_data[limidx,"dconf"] = tempdat$fobs*as.numeric(predict(confdmod, type="response"))
} else{
  cens_data[limidx,"nconf"] = 0
  cens_data[limidx,"dconf"] = 0
}
```


Next, if the sum of follow-up weights for those that are censored are greater than 1, then we will create two separate models. The first is a logistic model where the outcome is censored status (0/1), and the predictors include `outtime` modeled non-linearly with a restricted cubic spline. `fu_weight` takes either a 0 or 1, and we use that weight in the model with the argument, `weight=fu_weight`. The second model is also a logistic model of censored status, but includes not only `outtime` (as a non-linear term), but also lagged radon, cumulative radon, and smoker status. Again, for rows that correspond to this current 0.1 regime, we substitute `ncens` and `dcens` with predictions from the respective models when the observation is at follow-up time and exposure is at work. Otherwise, 0's are imputed.


```{r}
if(sum(tempdat[tempdat$fu_weight==1,"cens"]) > 1){
  summary(censnmod <- glm(cens ~ outtime + rcspline.eval(outtime, knots=outkn), data = tempdat, weight=fu_weight, family=binomial()))
  summary(censdmod <- glm(cens ~  outtime + rcspline.eval(outtime, knots=outkn) + rad_lag1 + cum_rad_lag1 + smoker, data = tempdat, weight=fu_weight, family=binomial()))
  # only get non-zero predictions if "fu_weight" == 1 (eligible to be censored during follow-up)
  cens_data[limidx,"ncens"] = tempdat$fu_weight*as.numeric(predict(censnmod, type="response")) 
  cens_data[limidx,"dcens"] = tempdat$fu_weight*as.numeric(predict(censdmod, type="response")) 
} else{
  cens_data[limidx,"ncens"] = 0
  cens_data[limidx,"dcens"] = 0
}


  
```


### Loop through Regimes

Alternative to above, we can perform the steps above, but for each of the three regimes (0.1, 0.3, natural course) using a for-loop:

```{r}
for (l in limits){
  print(paste0(c("Regime: ",l)))
  limidx = which(cens_data$limit == l)
  tempdat = cens_data[limidx,]
  tempdat$mxl = tempdat$cum_rad_lag1 / (tempdat$durwork_lag1 + as.numeric(tempdat$outtime==1))
  outkn0 = attr(rcspline.eval(filter(tempdat, fobs==1)$outtime, nk = 4), "knots")
  outkn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$outtime, nk = 4), "knots")
  
  #yearkn0 = attr(rcspline.eval(filter(tempdat, fobs==1)$year, nk = 4), "knots")
  #agekn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$age, nk = 4), "knots")
  #yearkn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$year, nk = 4), "knots")
  #
  if(sum(tempdat[tempdat$fobs==1,"cens"]) > 10){
    summary(confnmod <- glm(cens ~ 1 , data = tempdat, weight=fobs, family=binomial()))
    summary(confdmod <- glm(cens ~ smoker, data = tempdat, weight=fobs, family=binomial()))
    # need to use base R operations here
    # only get non-zero predictions if "fobs" == 1 (eligible to be "censored" at baseline)
    cens_data[limidx,"nconf"] = tempdat$fobs*as.numeric(predict(confnmod, type="response"))
    cens_data[limidx,"dconf"] = tempdat$fobs*as.numeric(predict(confdmod, type="response"))
  } else{
    cens_data[limidx,"nconf"] = 0
    cens_data[limidx,"dconf"] = 0
  }
  if(sum(tempdat[tempdat$fu_weight==1,"cens"]) > 1){
    summary(censnmod <- glm(cens ~ outtime + rcspline.eval(outtime, knots=outkn), data = tempdat, weight=fu_weight, family=binomial()))
    summary(censdmod <- glm(cens ~  outtime + rcspline.eval(outtime, knots=outkn) + rad_lag1 + cum_rad_lag1 + smoker, data = tempdat, weight=fu_weight, family=binomial()))
    # only get non-zero predictions if "fu_weight" == 1 (eligible to be censored during follow-up)
    cens_data[limidx,"ncens"] = tempdat$fu_weight*as.numeric(predict(censnmod, type="response")) 
    cens_data[limidx,"dcens"] = tempdat$fu_weight*as.numeric(predict(censdmod, type="response")) 
  } else{
    cens_data[limidx,"ncens"] = 0
    cens_data[limidx,"dcens"] = 0
  }
}


```



## Create Final Weights

***QUESTIONS: what is dconf/nconf??***

We are now ready to create the final weights. We create a new data set called `combined_wtd_data`. This takes in our censored data (`cens_data`). We first create the variable `wtcontr` which takes the following: when we have the first occurrence for the participant exposed at work, then the weight will be $\frac{(1-censoringstatus)\times(1-nconf)}{(1-dconf)}$; when we have follow-up measurement for the participant exposed at work then the weight will be  $\frac{(1-censoringstatus)\times(1-ncens)}{(1-dcens)}$. We save these calculations as `wtcontr`, with the default weight of 1. We select key variables, to reduce the computational burden using the `select()` function from `dplyr`. Next, we group by `cloneid` and calculate the cumulative product of `wtcontr`. This becomes our *inverse probability weight*, `ipw`. Lastly, following [Cain et al.](https://pubmed.ncbi.nlm.nih.gov/21972433/), we truncate the weight at 10.


```{r}
combined_wtd_data <- cens_data %>% 
  mutate(wtcontr = 
           case_when(((fobs == 1) & (atwork==1)) ~ (1-cens)*(1-nconf)/(1-dconf),
                     ((fobs == 0) & (atwork==1)) ~ (1-cens)*(1-ncens)/(1-dcens),
                     .default=1)) %>%
    dplyr::select(id,cloneid,intime,outtime,dead,rad,wtcontr,rad,cum_rad,atwork,cens,limit, smoker) %>% # optional step here to reduce comp. burden
    group_by(cloneid) %>%
    mutate(ipw = cumprod(wtcontr)) %>%
   # weight truncation at 10.0 following Cain et al 2010 (should be a user option)
    mutate(ipw_trunc = pmin(ipw, 10.0)) %>%
  ungroup()
  
```


To visualize the weights, we can look at a brief histogram:

```{r}
hist(combined_wtd_data$ipw)
```


## Inverse-Probability-Weighted Cox Model


We will explore the effects of a limit versus the reference. We will calculate the robust variance at the `ID` level, but observations are done by the `cloneid` level. The robust variance will give valid standard errors.

We create a new variable called `limitf`, which is a factor, and comprises the three different regimes.

```{r}
combined_wtd_data$limitf = factor(combined_wtd_data$limit, 
                                  levels=c("Inf", "0.3", "0.1"),
                                  labels=c("Natural course", "Limit: 0.3", "Limit: 0.1"))

```


<!-- We execute the inverse probability-weighted Cox PH model using the `coxph()` function from the  `survival` package, but only for individuals that have an inverse probability weight greater than 0 (`data=filter(combined_wtd_data, ipw>0)`). We specify the arguments ` id=cloneid, weight=ipw, cluster=id`. -->

<!-- ```{r} -->
<!-- msm = coxph(Surv(intime, outtime, dead)~limitf, data=filter(combined_wtd_data, ipw>0),  -->
<!--             id=cloneid, weight=ipw, cluster=id, x=FALSE, y=FALSE) -->

<!-- ``` -->

We can  estimate an inverse probability-weighted survival curve for the risk under each limit/regime:

```{r}
tt0 = survfit(Surv(intime, outtime, dead)~limitf, data=combined_wtd_data, weight=ipw, id=cloneid)
tt0

```



## Final Results

We're ready to combine our results into a final data set, `results`. The variables are the following:

- `time`: time
- `ci0`: Risk (cumulative incidence) under the natural course regime
- `cip1`: Risk (cumulative incidence) under the 0.1 limit regime
- `cip3`: Risk (cumulative incidence) under the 0.3 limit regime
- `rd_exp1_10`: risk difference between 0.1 limit regime and natural course
- `rd_exp3_10`: risk difference between 0.3 limit regime and natural course
- `lrr_exp1_10`: log relative risk between 0.1 limit regime and natural course
- `lrr_exp3_10`: log relative risk between 0.3 limit regime and natural course


```{r}
results = data.frame(time=tt0$time, 
                     risk = 1-tt0$surv,
                     int = do.call(c,sapply(1:length(tt0$strata), function(x) rep(names(tt0$strata)[x], tt0$strata[x])))) %>%
  mutate(ci0 = case_when(
    int == "limitf=Natural course" ~ risk,
    .default = as.numeric(NA)),
    cip1 = case_when(
      int == "limitf=Limit: 0.1" ~ risk,
      .default = as.numeric(NA)),
    cip3 = case_when(
      int == "limitf=Limit: 0.3" ~ risk,
      .default = as.numeric(NA))) %>%
  dplyr::select(time, ci0, cip1, cip3) %>%
  rbind(data.frame(time=0, 
                   ci0=0, 
                   cip1=0, 
                   cip3=0)) %>%
  arrange(time) %>%
  na.locf() %>%
  mutate(
    rd_exp1_10 = cip1-ci0,
    rd_exp3_10 = cip3-ci0,
    lrr_exp1_10 = log(cip1/ci0),
    lrr_exp3_10 = log(cip3/ci0)) %>%
  dplyr::select(time, ci0, rd_exp1_10, rd_exp3_10, lrr_exp1_10, lrr_exp3_10)



```


We visualize these results:

```{r}

ggplot(aes(x=time), data=results) + 
  geom_line(aes(x=time, y=ci0, color="Natural course")) +
  geom_line(aes(x=time, y=ci0+rd_exp3_10, color="Limit: 0.3")) +
  geom_line(aes(x=time, y=ci0+rd_exp1_10, color="Limit: 0.1")) +
  scale_y_continuous(name="Cumulative incidence") +
  scale_x_continuous(name="Time", limits=c(0,20)) +
  scale_color_discrete(name="") +
  theme_classic() +
  theme(legend.position="inside", legend.position.inside = c(0.9, 0.15))+
  ggtitle(expression(paste("IPW estimates of cumulative incidence")))

```


### A Function

Alternatively, we can execute the same code above, but as a function, `ipw_risks()`. This improves reproducibility of the analysis:


```{r}
ipw_risks = function(data=miners){
  
  limits=c(Inf, 0.3, 0.1)
  #1b) create copy of the dataset for each limit
  clones <- do.call(rbind,
                    lapply(1:length(limits), function(x) {  
                      df = data                  
                      df$limit = as.numeric(limits[x])
                      df$cloneid = paste0(x, "_", df$id)
                      df                    
                    }
                    ))
  #2) artificially censor data (keeping first observation that is censored)
  cens_data <- clones %>%
    group_by(cloneid) %>%
    mutate(
      cens = pmin(1,cumsum(rad > limit)),
      drop = pmin(2, cumsum(cens))
    ) %>%
    group_by() %>%
    filter(
      drop < 2
    )
  
  # 3) create 1/0 weights to use for confounding/censoring during follow-up (alternative is to create new datasets)
  cens_data <- group_by(cens_data, cloneid) %>%
    mutate(
      one = 1,
      nobs = cumsum(one),
      fobs____ = as.numeric(nobs == 1)
    ) %>%
    mutate(
      conf_weight = as.numeric(nobs  == 1), # this duplicates fobs____ but is kept for clarity
      fu_weight = as.numeric((nobs  > 1) & (atwork == 1)),    
      dconf = 0,
      dcens = 0,
      intervened = 0,
    ) %>%
    select(-c(one, nobs)) %>%
    ungroup()
  # check: which(tapply(data$x, data$id, max) < limit & tapply(data$atwork, data$id, sum)>3)[2] # index 427
  # check: names(tapply(data$x, data$id, max))[427]
  # check: print(select(cens_data, c(id, x, limit, conf_weight, fu_weight, cens, fobs____)) %>% filter(id==427), n=50)
  # check: print(select(filter(data, id==427), c(id, x, atwork)))
  
  
  #4) fit censoring models (can include pre-baseline exposure in practice)
  
  # fit models if there is more than a small amount of censoring (seems to work either way, but this avoids convergence problems)
  for (l in limits){
    limidx = which(cens_data$limit == l)
    tempdat = cens_data[limidx,]
    tempdat$mxl = tempdat$cum_rad_lag1 / (tempdat$durwork_lag1 + as.numeric(tempdat$outtime==1))
    outkn0 = attr(rcspline.eval(filter(tempdat, conf_weight==1)$outtime, nk = 4), "knots")
    outkn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$outtime, nk = 4), "knots")
    
    #yearkn0 = attr(rcspline.eval(filter(tempdat, conf_weight==1)$year, nk = 4), "knots")
    #agekn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$age, nk = 4), "knots")
    #yearkn = attr(rcspline.eval(filter(tempdat, fu_weight==1)$year, nk = 4), "knots")
    #
    if(sum(tempdat[tempdat$conf_weight==1,"cens"]) > 10){
      summary(confnmod <- glm(cens ~ 1 , data = tempdat, weight=conf_weight, family=binomial()))
      summary(confdmod <- glm(cens ~ smoker, data = tempdat, weight=conf_weight, family=binomial()))
      # need to use base R operations here
      # only get non-zero predictions if "conf_weight" == 1 (eligible to be "censored" at baseline)
      cens_data[limidx,"nconf"] = tempdat$conf_weight*as.numeric(predict(confnmod, type="response"))
      cens_data[limidx,"dconf"] = tempdat$conf_weight*as.numeric(predict(confdmod, type="response"))
    } else{
      cens_data[limidx,"nconf"] = 0
      cens_data[limidx,"dconf"] = 0
    }
    if(sum(tempdat[tempdat$fu_weight==1,"cens"]) > 1){
      summary(censnmod <- glm(cens ~ outtime + rcspline.eval(outtime, knots=outkn), data = tempdat, weight=fu_weight, family=binomial()))
      summary(censdmod <- glm(cens ~  outtime + rcspline.eval(outtime, knots=outkn) + rad_lag1 + cum_rad_lag1 + smoker, data = tempdat, weight=fu_weight, family=binomial()))
      # only get non-zero predictions if "fu_weight" == 1 (eligible to be censored during follow-up)
      cens_data[limidx,"ncens"] = tempdat$fu_weight*as.numeric(predict(censnmod, type="response")) 
      cens_data[limidx,"dcens"] = tempdat$fu_weight*as.numeric(predict(censdmod, type="response")) 
    } else{
      cens_data[limidx,"ncens"] = 0
      cens_data[limidx,"dcens"] = 0
    }
  }
  # create final weights
  combined_wtd_data <- cens_data %>% 
    mutate(
      wtcontr = case_when(
        # weight: (stabilized) inverse probability of NOT being censored
        # note regarding unstabilized weights: Cain et al 2010 show that weight stabilization is not guaranteed to reduce variance
        ((fobs____ == 1) & (atwork==1)) ~ (1-cens)*(1-nconf)/(1-dconf),
        ((fobs____ == 0) & (atwork==1)) ~ (1-cens)*(1-ncens)/(1-dcens),
        .default=1
      )
    ) %>%
    #select(id,cloneid,intime,outtime,dead,rad,wtcontr,rad,cum_rad,atwork,cens,limit, smoker) %>% # optional step here to reduce comp. burden
    group_by(cloneid) %>%
    mutate(
      ipw = cumprod(wtcontr)
    ) %>%
    group_by() %>%
    mutate(
      # weight truncation at 10.0 following Cain et al 2010 (should be a user option)
      ipw_trunc = pmin(ipw, 10.0)
    )
  
  # check: summary(select(combined_wtd_data, c(ipw, ipw_trunc)))
  # check: print(select(combined_wtd_data, c(id, age, atwork, x, cens, wtcontr, ipw, ipw_trunc)) %>% filter(id==427), n=10)
  
  # check mean weights by intervention
  # note mean is taken across all possible observations, even those with weights = 0
 # N = nrow(data)
 # Nid = length(unique(data$id))
 # wtdx = data.frame(
 #   limit = limits,
 #   mean_cumx = do.call(c,lapply(limits, function(x) mean(combined_wtd_data[combined_wtd_data$limit == x  & combined_wtd_data$cens == 0,]$cumx))),
 #   n_conf = do.call(c,lapply(limits, function(x) sum(combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$fobs____==1,]$cens))),
 #   n_cens = do.call(c,lapply(limits, function(x) sum(combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$fobs____==0,]$cens))),
 #   n_d1w = do.call(c,lapply(limits, function(x) sum(combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$cens == 0,]$ipw*combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$cens == 0,]$d1))),
 #   n_d2w = do.call(c,lapply(limits, function(x) sum(combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$cens == 0,]$ipw*combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$cens == 0,]$d2))),
 #   mean_ipw = do.call(c,lapply(limits, function(x) mean(combined_wtd_data[combined_wtd_data$limit == x & combined_wtd_data$cens == 0,]$ipw)))
 # )
  

  ##########################################################################################
  # marginal structural (IP weighted) Cox model: effects of limit vs. ref
  # robust variance is done by ID level, but observations are done by CLONEID level
  # the robust variance gives valid standard errors for the hazard ratio
  ##########################################################################################
  combined_wtd_data$limitf = factor(combined_wtd_data$limit, levels=c("Inf", "0.3", "0.1"), labels=c("Natural course", "Limit: 0.3", "Limit: 0.1"))
  msm = coxph(Surv(intime, outtime, dead)~limitf, data=filter(combined_wtd_data, ipw>0), 
               id=cloneid, weight=ipw, cluster=id, x=FALSE, y=FALSE)
  ##########################################################################################
  # IP weighted survival curve: risk under each limit
  ##########################################################################################
  # Survival curves for each limit
   tt0 = survfit(Surv(intime, outtime, dead)~limitf, data=combined_wtd_data, weight=ipw, id=cloneid)
  # return a vector of risk differences and log-risk ratios at time t=endtime
  
  results = data.frame(
    time=tt0$time, 
    risk = 1-tt0$surv,
    int = do.call(c,sapply(1:length(tt0$strata), function(x) rep(names(tt0$strata)[x], tt0$strata[x])))
  ) %>%
    mutate(
      ci0 = case_when(
        int == "limitf=Natural course" ~ risk,
        .default = as.numeric(NA)
      ),
      cip1 = case_when(
        int == "limitf=Limit: 0.1" ~ risk,
        .default = as.numeric(NA)
      ),
      cip3 = case_when(
        int == "limitf=Limit: 0.3" ~ risk,
        .default = as.numeric(NA)
      )
    ) %>%
    select(time, ci0, cip1, cip3) %>%
    rbind(data.frame(time=0, ci0=0, cip1=0, cip3=0)) %>%
    arrange(time) %>%
    na.locf() %>%
    mutate(
      rd_exp1_10 = cip1-ci0,
      rd_exp3_10 = cip3-ci0,
      lrr_exp1_10 = log(cip1/ci0),
      lrr_exp3_10 = log(cip3/ci0),
    ) %>%
    select(
      time, ci0, rd_exp1_10, rd_exp3_10, lrr_exp1_10, lrr_exp3_10
    )
    
  results
}

```




## Bootstrapped Confidence Intervals

In order to get confidence intervals, we will use the non-parametric bootstrap. It is generally recommended to use at least 200+ bootstrap samples for valid standard errors (and at least 1000 for  percentile-based confidence intervals). In the interest of time, we will only use 10 bootstrap resamples.


First, we must create the function for estimating effects, which will be fed into the `boot()` function. This function, `ipw_effectestimates()` takes the following arguments:

- @param `data` baseline data (created above)
- @param `index` keep to NULL in order to use original data
- @param `fdata` the data set of interest, here is it our `miners` data set
- @param `endtime` maximum number of time periods
- @param `seed`  seed value we input so that the results are reproducible. 

```{r}

ipw_effectestimates <- function(data=baseline_data, index=NULL, fdata=miners, endtime=10, seed=NULL){
  # for bootstrap samples, when needed: first take a random sample of the 
  #  study IDs, with replacement
  # 0) preprocessing: take bootstrap sample if bootstrapping, otherwise use the observed data
  if(is.null(index)){
    # if index is NULL, then just use original data and do nothing here
  } else{
    # bootstrap sample of the baseline data using the vector 'index'
    data = slice(data, index)
    # resample from the full data according to the id variable in the baseline data
    idindex = tibble(reps = table(data$id), id = as.numeric(names(table(data$id))))
    #fdata = merge(idindex, fdata, by='id', sort=FALSE)
    #fdata = right_join(fdata, idindex, by=c('id'))
    fdata = left_join(idindex, fdata, by=c('id'))
    #sortindex = rep(1:dim(fdata)[1], times=fdata$reps)
    
    idxlist = sapply(1:dim(fdata)[1], function(x) rep(x, fdata$reps[x]))
    maxcopies = max(sapply(idxlist, length))
    sortindex_full = do.call(c,lapply(1:maxcopies, function(z) sapply(idxlist, function(x) x[z])))
    sortindex = sortindex_full[!is.na(sortindex_full)]
    fdata = slice(fdata, sortindex)
    fdata$oldid = fdata$id
    fdata$id = cumsum(fdata$intime==0) # need to keep track of new ID or bootstrap observations will get combined
  }
  ipw_boot = ipw_risks(data=fdata)
  unlist(tail(ipw_boot[ipw_boot$time<=endtime,], n=1))
}
```

Perform the bootstrapping:

```{r}
nbootsamples = 10
system.time(boot_samples_ipw <- boot(miners, statistic = ipw_effectestimates, R = nbootsamples, endtime=10))
boot_samples_ipw$t0
```

Finally we can obtain the point estimates and confidence intervals:

```{r}
# point estimate and confidence intervals
est_ipw = boot_samples_ipw$t0
lci_ipw =  boot_samples_ipw$t0 - 1.96*apply(boot_samples_ipw$t, 2, sd)
uci_ipw =  boot_samples_ipw$t0 + 1.96*apply(boot_samples_ipw$t, 2, sd)

cbind.data.frame(est_ipw,
                        lci_ipw,
                        uci_ipw) %>%
  knitr::kable(caption="Risk Differences/Log Relative Risks and 95% Bootstrapped Confidence Intervals (R=10) Under Different Treatment Regimes at t=10 Using Inverse Probability Weighting (IPW)")

```



