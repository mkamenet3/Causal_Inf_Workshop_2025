---
title: "Longitudinal G-Formula"
author: "A. Keil and M.Kamenetsky"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,cache=TRUE)
```


```{r, class.source = 'fold-show'}
#Load in libraries
library(ggplot2) #for plotting
library(dplyr) #for data cleaning
library(survival) #for survival analysis
library(boot) #for bootstrapping

```

   
## Research Questions:

To estimate the risk difference of four different treatment regimes of radium reduction on uranium mine workers.




```{r}
library(ggdag)
longdag <- ggdag::dagify(L2 ~ L1,
                         A1 ~ L1,
                         L2 ~ A1,
                         A2 ~ A1,
                         A2 ~ L2,
                         Y2 ~ A2,
                         Y2 ~ A1,
                         Y2 ~ L1,
                         Y2 ~ L2)

#set.seed(2)
set.seed(18)
ggdag::ggdag(longdag)+
  theme_void() +
  ggtitle("Example: Longitudinal DAG")
```






## Learning Objectives:

By the end of this tutorial, you will be able to:


- Complete a parametric g-formula analysis for longitudinal data using `R` software
- Estimate different joint effects under different reduction scenarios in a longitudinal data set




## Key Points:


The Monte Carlo algorithm can be used to estimate $E(Y^g_2)$ or $p(y_2^g)$:

1) Sample with replacement from the target population at time $k=0$, $\hat{p}(l_1) = \hat{p}_n(l_1)$ (**pseudo-population**, $N^g$)
2) Set $A_1$ equal to $g$ for everyone in the pseudo-population
3) Simulate values from $L_2$ from $p(l_2|g, l_1)$
  - For example, a Bernoulli distribution with $\mu = p(l_2|g,l_1)$
4) Simulate values of $Y_2$ from $p(Y_2|g, l_1, l_2)$
5) $E(Y^g)$ is the mean of $Y_2$ in the simulated pseudo-population


# Uranium Mine Workers Data Example: Longitudinal G-Formula

Consider a longitudinal study where you have two data measured at two (or more) time points. If we want to estimate **TODO**, we can explore different realizations under different regimes, such as "always exposed", "never exposed", "natural course". To do so, we can simulate individuals through time using the directed acyclic graph (DAG)



The target population consists of uranium miners. We would like to estimate the risk (or cumulative incidence) from 0-20 years. We want to estimate the following regimes:

- No intervention ("natural course")
- 0.3 x 1000 pCi/L-year limit ("slightly below current standard")
- 0.1 x 1000 pCi/L-year limit ("well below current standard")
- No exposure ("attributable risk")

These are considered to be *dynamic regimes*, in that at work, exposure will be below the limit and unexposed otherwise.



We load in the data set `miners` using `read.csv()` function. We pipe (`%>%`) that data to the `mutate()` function to create a new variable (`lograd`), which we will use later. This variable takes the natural logarithm of the radium values when participants were at work (`atwork==1`) and is missing otherwise, as stated in the `ifelse()` function. We briefly explore a summary of the data:

```{r, class.source = 'fold-show'}
miners <- read.csv("2018_ISEE_causal-master/analyses/data/miners.csv") %>%
  mutate(lograd = ifelse(atwork==1, log(rad), NA))
str(miners)

```


By using the `str()` command, we have the following 13 variables:

- `id`: a unique identifier for each individual 
- `intime`: time during which participant is at work
- `outtime`: time during which participant is not at work
- `dead`: binary indicator for dead (1) or not dead (0)
- `rad`: radium measurement
- `rad_lag1`: radium measurement lagged by 1 time period
- `cum_rad`: cumulative radium exposure
- `cum_rad_lag1`: cumulative radium exposure lagged by 1 time period
- `atwork`: binary indicator of if exposure took place at work (1) or not at work (0)
- `leavework`: binary indicator of if exposure was on leave from work (1) or not on leave from work (0) **TODO**
- `durwork`: binary indicator of if exposure occurred during work (1) or not (0)
- `durwork_lag1`: binary indicator of if exposure occurred during work (1) or not (0) lagged by 1 time period
- `smoker`: binary indicator of individual was a smoker (1) or not a smoker (0)
- `lograd`: natural logarithm of the radium measurement when at work


```{r}
summary(miners[, 2:13])


```



First, extract baseline data at `intime=0` using the `filter()` function. We also create a new variable called `lograd` using the `mutate()` function which takes the natural logarithm of the radium measurement where participants were at work (`atwork==1`) and are missing (`NA`) otherwise. We set `N` to be equal to the number of observations (or `nrow()`):


```{r, class.source = 'fold-show'}
baseline_data <- filter(miners, intime==0) 
N = nrow(baseline_data)

```

Sample with replacement over 20000 realizations (`mc_iter`). To do this, we first set the seed so the analysis can be reproduced (`set.seed(12325)`). We then use the `slice()` function from the `dplyr` package to perform the sampling `mc_iter` times, with replacement (`replace=TRUE`).

```{r, class.source = 'fold-show'}
# large sample from observed baseline data
mc_iter = 20000
set.seed(12325)
mcdata <- slice(baseline_data, sample(1:N, size = mc_iter, replace=TRUE))

```


We develop three different models:

1) Pooled linear model for (log) radium exposure while at work. This is model is the annual log-radium exposure: $rad_i = \beta_0 + \beta_1 outtime_i + \beta_2 cum\_rad\_lag1_i + \beta_3 smoker_i + \varepsilon_i$
2) Pooled logistic model for leaving work (at the end of the year) while at work (where $p_i$ is the probability of leaving work). This model the time-varying confounder of employment: $\log(\frac{p_i}{1-p_i}) = \beta_0 + \beta_1outtime_i + \beta_2 outtime_i^2 + \beta_3 rad\_lag1_i + \beta_4 cum\_rad\_lag1 + \beta_5 smoker_i$
3) Pooled logistic model for death  (where $p_i$ is the probability of death). This models the time-varying outcome of death: $\log(\frac{p_i}{1-p_i}) = \beta_0 + \beta_1 outtime_i + \beta_2 atwork_i + \beta_3 cum\_rad_i + \beta_4 cum\_rad^2_i + \beta_4 smoker_i + \beta_5 cum\_rad_i*smoker_i$


For model 1, we filter the data such that we are only looking at when participants were at work (`filter(miners, atwork==1)`)

```{r, class.source = 'fold-show'}
#Model 1: pooled linear model for log(exposure), while at work
mod_e = lm(lograd ~ outtime + cum_rad_lag1 + smoker, data=filter(miners, atwork==1))
```


For model 2, we filter the data to where participants where either at work or were on leave from work (`filter(miners, atwork==1 | leavework==1)`):

```{r, class.source = 'fold-show'}

# pooled logistic model for leaving work (at the end of the year), while at work
mod_w = glm(leavework ~ outtime + I(outtime^2) + rad_lag1 + cum_rad_lag1 + smoker, data=filter(miners, atwork==1 | leavework==1), family=binomial())
 

```


For model 3, we do not filter the data and use the whole data set of miners:

```{r, class.source = 'fold-show'}
mod_d = glm(dead ~ outtime + atwork + cum_rad + I(cum_rad*cum_rad) + smoker + smoker*cum_rad, data=miners, family=binomial(link=logit))
  
```


# Treatment Regimes

## Natural Course

Now that we have our three models, we will use them to create predictions under the different treatment regimes. First, we explore the natural course progression. We will do this manually first, and then create a function to make this more reproducible. We start with the pseudo-cohort at baseline we created above (`mcdata`). We set the end time for our hypothetical cohort to be 20 years, which is the follow-up time for which we want to follow this cohort. For each pseudo-person in our new pseudo-cohort, we create a new identifier, `gfid`. For each of the variables in the original data set, we set initial values. 

For the linear exposure model, we also set the error variance as the sum of the squared residuals divided by the residual degrees of freedom. In order to prevent prediction outside of the observed range, we set a limit on the log exposure as `max_e = 7.321276`.

```{r, class.source = 'fold-show'}
N <- nrow(mcdata)
ncols <- ncol(mcdata)
pseudo_cohort <- slice(mcdata,rep(1:N, each=20))
#create a new identifier
pseudo_cohort$gfid <- rep(seq(1, N, 1), each=20) # new id for each pseudo individual
# initialize values
pseudo_cohort$intime <- rep(seq(0, (20-1), 1), times=N)
pseudo_cohort$outtime <- pseudo_cohort$intime+1
pseudo_cohort$atwork <- 1
pseudo_cohort$durwork <- 1
pseudo_cohort$leavework <- 0
pseudo_cohort$rad <- 0
pseudo_cohort$rad_lag1 <- 0
pseudo_cohort$cum_rad_lag1 <- 0
pseudo_cohort$durwork_lag1 <- 0
pseudo_cohort$dead <- 0
pseudo_cohort$cum_hazard <- 0 # new cumulative hazard
pseudo_cohort$cum_incidence <- 0 # new cumulative incidence

# set error variance for exposure model
var_e = sum(mod_e$residuals^2)/mod_e$df.residual 


# limits on log exposure (keep simulated values in range of observed)
max_e = 7.321276

#for the natural course, there is no intervention so we set it to NULL
intervention <- NULL

head(pseudo_cohort)#check out the first 6 observations of the pseudo-cohort and confirm that initial values were initialized correctly

```


Now that the parameters have been set, we simulate time-varying values using a series of for-loops. The outer loop loops over each `t` time period in time periods 1 through 20. 

Depending on which model (leaving work) or if exploring lagged variables, we perform appropriate accounting (ex: starting everyone at work in first period). Then we use the respective three models above (`mod_e`, `mod_w`, `mod_d`) to predict and generate random variables using `rbinom()` or `rnorm()` functions to simulate responses based on those predicted models.

```{r, class.source = 'fold-show'}
set.seed(12325)
for(t in 1:20){
  # index that keeps track of time
  idx = which(pseudo_cohort$outtime == t)
  #update all values of lagged variables (easy to forget!)
  if(t > 1){
    pseudo_cohort[idx,'cum_rad_lag1'] = pseudo_cohort[idx-1,'cum_rad']
    pseudo_cohort[idx,'rad_lag1'] = pseudo_cohort[idx-1,'rad']
  }    
  ############ 
  # leaving work (time varying confounder)
  ############
  if(t==1){
    widx = 1:length(idx) # everyone is at work at first time point
  } 
  if(t > 1){
    widx = which(pseudo_cohort[idx-1,'atwork']==1)
    # index keeping track of which workers still work
    pseudo_cohort[idx[widx],'leavework'] <- rbinom(n=length(widx), size=1, 
                                                   prob=predict(mod_w, newdata = pseudo_cohort[idx[widx],], 
                                                                type = 'response'))
    # if worker didn't leave, then assume stay at work
    pseudo_cohort[idx,'atwork'] <- (pseudo_cohort[idx,'leavework']==0 & pseudo_cohort[idx-1,'atwork']==1)
    # update at work index to account for workers who left
    widx = which(pseudo_cohort[idx,'atwork']==1)
    pseudo_cohort[idx,'durwork'] <- pseudo_cohort[idx-1,'durwork'] + pseudo_cohort[idx,'atwork']
    pseudo_cohort[idx,'durwork_lag1'] <- pseudo_cohort[idx-1,'durwork']
  }
  ############
  # exposure/interventions
  ############
  # exposure is assumed log-normally distributed (=predicted value plus draw from the residuals)
  meanlogr = predict(mod_e, newdata = pseudo_cohort[idx[widx],])
  logr = meanlogr + rnorm(n=length(widx), mean=0, sd=sqrt(var_e))
  pseudo_cohort[idx[widx],'rad'] <- exp(pmin(log(max_e), logr))
  # exposure under interventions
  if(typeof(intervention) != "NULL"){
    if(is.numeric(intervention)){
      # static, deterministic intervention, e.g. set exposure = 0
      pseudo_cohort[idx,'rad'] <- intervention
    } else{
      if(is.character(intervention)){
        # dynamic interventions are defined by character strings: e.g. 'rad>0.3' means we intervene to prevent radon exposure from being > 0.3
        #  this line creates an index of rows at the current time which are in violation of the intervention
        #  exposure will be redrawn for these observations until they are in compliance.
        #  This is equivalent to drawing from a truncated log-normal distribution
        viol_idx <- with(pseudo_cohort[idx,], which(eval(parse(text=intervention))))
        while(length(viol_idx)>0){
          # accept/rejection sampling to get draws from truncated log-normal
          # this is how we (I) assume an intervention would be implemented, but
          # we could imagine other ways (e.g. a hard cap on exposure)
          meanlogr = predict(mod_e, newdata = pseudo_cohort[idx[viol_idx],])
          lograd = meanlogr + rnorm(n=length(viol_idx), mean=0, sd=sqrt(var_e))
          pseudo_cohort[idx[viol_idx],'rad'] <- exp(lograd)
          # check whether any are still in violation of the distribution, if so, repeat loop
          viol_idx <- with(pseudo_cohort[idx,], which(eval(parse(text=intervention))))
        }}} # end dynamic intervention
  } # end all interventions on exposure
  if(t > 1){
    pseudo_cohort[idx,'cum_rad'] = pseudo_cohort[idx-1,'cum_rad'] + pseudo_cohort[idx,'rad']
  } else pseudo_cohort[idx,'cum_rad'] = pseudo_cohort[idx,'rad']
  ############
  # death (simulate discrete hazard as in Taubman et al 2009, rather than 1/0 as in Keil et al 2014)
  #  see note at bottom of program about late entry - in the case of late entry
  #  the typical approach is to simulate actual death (1/0 variable) and then
  #  estimate survival in the pseudo-population using a survival curve estimator
  #  that can account for late entry
  ############
  pseudo_cohort[idx,'dead'] = predict(mod_d, newdata = pseudo_cohort[idx,], type = 'response')
  if(t > 1){
    # product limit estimator of cumulative incidence (note this is applied on the individual basis)
    pseudo_cohort[idx,'cum_incidence'] = pseudo_cohort[idx-1,'cum_incidence'] + 
      (1-pseudo_cohort[idx-1,'cum_incidence'])*pseudo_cohort[idx,'dead']
  } else{
    pseudo_cohort[idx,'cum_incidence'] = pseudo_cohort[idx,'dead']
  }
  #alternative estimator of the cumulative incidence: Breslow estimator
  #if(t > 1){
  #  # Kaplan-meier estimator of cumulative incidence (note this is applied on the individual basis)
  #  pseudo_cohort[idx,'cum_hazard'] = pseudo_cohort[idx-1,'cum_hazard'] + pseudo_cohort[idx,'dead']
  #} else{
  #  pseudo_cohort[idx,'cum_hazard'] = pseudo_cohort[idx,'dead']
  #}
  #  pseudo_cohort[idx,'cum_incidence'] = 1-exp(-pseudo_cohort[idx,'cum_hazard'])
  # could also use Breslow estimator, but need aalen-johansen estimator if there are competing risks
  # the 'cum_incidence' variable is an estimate of the individual risk. We then
  # average that over the population below to get the marginal risk
} # end time loop
head(pseudo_cohort)#check out first 6 observations with new predicted values
```

From our pseudo-cohort under the natural course regime, we want to calculate the cumulative incidence for each year, 1 through 20. To do so, we take the mean cumulative incidence in each year group (`outtime`) across all individuals in the pseudo-cohort. We print our results using the `kable()` function from the `knitr` package


```{r, class.source = 'fold-show'}

cuminc_nc0 <- with(pseudo_cohort, tibble(time=seq(0,20, 1), ci=c(0, tapply(cum_incidence, outtime, mean)))) 
knitr::kable(cuminc_nc0, caption = "Cumulative incidence: unexposed regime (hard code)")


```


For the other 3 regimes, we must do the same thing. In order to make things more reproducible, we will create a function called `gformula_risks()` which will allow us to perform the same procedure. The parameters to the function are:

- @param `intervention` Intervention or treatment regime. Options are `NULL`, 0, `rad>0.3`, or `rad>0.1`, as defined above.
- @param `pseudo_cohort_bl` Pseudo-cohort at baseline. We create this object above and have called it `mcdata`.
- @param `endtime` Number of total years for the pseudo-cohort to be followed. In this study, we set `endtime=20`
- @param `mod_e` Pooled linear model for (log) radium exposure while at work. This is model is the annual log-radium exposure (defined above). This model is also created above and stored as the object `mod_e`.
- @param `mod_w` Pooled logistic model for leaving work (at the end of the year) while at work. This models the time-varying confounder of employment. This model is also created above and stored as the object `mod_w`.
- @param `mod_d` Pooled logistic model for death. This models the time-varying outcome of death. This model is also created above and stored as the object `mod_d`.
- @param `seed` Seed value we input so that the results are reproducible. 





```{r, class.source = 'fold-show'}
gformula_risks <- function(intervention=NULL, pseudo_cohort_bl, 
                           endtime, mod_e, mod_w, mod_d, seed=NULL){
  N <- nrow(pseudo_cohort_bl)
  ncols <- ncol(pseudo_cohort_bl)
  pseudo_cohort <- slice(pseudo_cohort_bl,rep(1:N, each=endtime))
  pseudo_cohort$gfid <- rep(seq(1, N, 1), each=endtime) 
  pseudo_cohort$intime <- rep(seq(0, (endtime-1), 1), times=N)
  pseudo_cohort$outtime <- pseudo_cohort$intime+1
  pseudo_cohort$atwork <- 1
  pseudo_cohort$durwork <- 1
  pseudo_cohort$leavework <- 0
  pseudo_cohort$rad <- 0
  pseudo_cohort$rad_lag1 <- 0
  pseudo_cohort$cum_rad_lag1 <- 0
  pseudo_cohort$durwork_lag1 <- 0
  pseudo_cohort$dead <- 0
  pseudo_cohort$cum_hazard <- 0 
  pseudo_cohort$cum_incidence <- 0 
  var_e = mean(mod_e$residuals^2) 
  set.seed(seed)
  max_e = 7.321276
  for(t in seq(1, endtime, 1)){
    idx = which(pseudo_cohort$outtime == t)
    if(t > 1){
      pseudo_cohort[idx,'cum_rad_lag1'] = pseudo_cohort[idx-1,'cum_rad']
      pseudo_cohort[idx,'rad_lag1'] = pseudo_cohort[idx-1,'rad']
    }    
    if(t==1) widx = 1:length(idx) 
    if(t > 1){
      widx = which(pseudo_cohort[idx-1,'atwork']==1)
      pseudo_cohort[idx[widx],'leavework'] <- rbinom(n=length(widx), size=1, 
                                                     prob=predict(mod_w, newdata = pseudo_cohort[idx[widx],], 
                                                                  type = 'response'))
      pseudo_cohort[idx,'atwork'] <- (pseudo_cohort[idx,'leavework']==0 & pseudo_cohort[idx-1,'atwork']==1)
      widx = which(pseudo_cohort[idx,'atwork']==1)
      pseudo_cohort[idx,'durwork'] <- pseudo_cohort[idx-1,'durwork'] + pseudo_cohort[idx,'atwork']
      pseudo_cohort[idx,'durwork_lag1'] <- pseudo_cohort[idx-1,'durwork']
    }
    meanlogr = predict(mod_e, newdata = pseudo_cohort[idx[widx],])
    logr = meanlogr + rnorm(n=length(widx), mean=0, sd=sqrt(var_e))
    pseudo_cohort[idx[widx],'rad'] <- exp(pmin(log(max_e), logr))
    if(typeof(intervention) != "NULL"){
      if(is.numeric(intervention)){
        pseudo_cohort[idx,'rad'] <- intervention
      } else{
        if(is.character(intervention)){
          viol_idx <- with(pseudo_cohort[idx,], which(eval(parse(text=intervention))))
          while(length(viol_idx)>0){
            meanlogr = predict(mod_e, newdata = pseudo_cohort[idx[viol_idx],])
            lograd = meanlogr + rnorm(n=length(viol_idx), mean=0, sd=sqrt(var_e))
            pseudo_cohort[idx[viol_idx],'rad'] <- exp(lograd)
            viol_idx <- with(pseudo_cohort[idx,], which(eval(parse(text=intervention))))
          }}}
    } 
    if(t > 1){
      pseudo_cohort[idx,'cum_rad'] = pseudo_cohort[idx-1,'cum_rad'] + pseudo_cohort[idx,'rad']
    } else pseudo_cohort[idx,'cum_rad'] = pseudo_cohort[idx,'rad']
    pseudo_cohort[idx,'dead'] = predict(mod_d, newdata = pseudo_cohort[idx,], type = 'response')
    if(t > 1){
      pseudo_cohort[idx,'cum_incidence'] = pseudo_cohort[idx-1,'cum_incidence'] + 
        (1-pseudo_cohort[idx-1,'cum_incidence'])*pseudo_cohort[idx,'dead']
    } else{
      pseudo_cohort[idx,'cum_incidence'] = pseudo_cohort[idx,'dead']
    }
  } 
  pseudo_cohort
}



```


We perform the simulation under the natural course again using the `gformula_risks()` function, and store the results from the function call into the object `nc` (or natural course).

```{r, class.source = 'fold-show'}
nc = gformula_risks(intervention=NULL, pseudo_cohort_bl=mcdata, endtime=20, 
                    mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, seed=12325)
gf_cuminc_nc <- with(nc, tibble(time=seq(0,20, 1), ci=c(0, tapply(cum_incidence, outtime, mean)))) 
knitr::kable(gf_cuminc_nc, caption = "Cumulative incidence: natural course regime")





```


We perform the same analysis under the remaining three regimes:

```{r, class.source = 'fold-show'}
unex = gformula_risks(intervention=0, 
                      pseudo_cohort_bl=mcdata, 
                      endtime=20, 
                      mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                      seed=12325)
gf_cuminc_unex <- with(unex, tibble(time=seq(0,20, 1), ci=c(0, tapply(cum_incidence, outtime, mean)))) 
knitr::kable(gf_cuminc_unex, caption = "Cumulative incidence: unexposed regime")

exp3 = gformula_risks(intervention='rad>0.3', 
                      pseudo_cohort_bl=mcdata,
                      endtime=20, 
                      mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                      seed=12325)
gf_cuminc_exp3 <- with(exp3, tibble(time=seq(0,20, 1), ci=c(0, tapply(cum_incidence, outtime, mean)))) 
knitr::kable(gf_cuminc_exp3, caption = "Cumulative incidence: radium>0.3 exposure regime")

exp1 = gformula_risks(intervention='rad>0.1', 
                      pseudo_cohort_bl=mcdata, 
                      endtime=20, 
                      mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                      seed=12325)
gf_cuminc_exp1 <- with(exp1, tibble(time=seq(0,20, 1), ci=c(0, tapply(cum_incidence, outtime, mean)))) 
knitr::kable(gf_cuminc_exp1, caption = "Cumulative incidence: radium>0.1 exposure regime")


```


Instead of using a table, we can visualize these results:

```{r, class.source = 'fold-show'}
ggplot() +
  geom_line(aes(x=time, y=ci, color="Natural course"), data=gf_cuminc_nc) +
  geom_line(aes(x=time, y=ci, color="Unexposed"), data=gf_cuminc_unex ) +
  geom_line(aes(x=time, y=ci, color="Limit: 0.3"), data=gf_cuminc_exp3 ) +
  geom_line(aes(x=time, y=ci, color="Limit: 0.1"), data=gf_cuminc_exp1 ) +
  scale_y_continuous(name="Cumulative incidence") +
  scale_x_continuous(name="Time", limits=c(0,20)) +
  scale_color_discrete(name="") +
  theme_classic() +
  theme(legend.position = c(0.99,0.01),legend.justification = c(1,0))+
  ggtitle(expression(paste("G-formula estimates of cumulative incidence")))


```


To compare to observed incidence, we can use the `survfit()` function from the `survival` package:

```{r, class.source = 'fold-show'}
obscifit <- survfit(Surv(intime, outtime, dead)~1, data=miners)
obsnc <- tibble(time=obscifit$time, ci=1-obscifit$surv)


```


```{r}
ggplot() +
  geom_line(aes(x=time, y=ci, color="Natural course"), data=gf_cuminc_nc) +
  geom_line(aes(x=time, y=ci, color="Unexposed"), data=gf_cuminc_unex ) +
  geom_line(aes(x=time, y=ci, color="Limit: 0.3"), data=gf_cuminc_exp3 ) +
  geom_line(aes(x=time, y=ci, color="Limit: 0.1"), data=gf_cuminc_exp1 ) + 
  geom_line(aes(x=time, y=ci, color="Observed"), data=obsnc) +
  scale_y_continuous(name="Cumulative incidence") +
  scale_x_continuous(name="Time", limits=c(0,20)) +
  scale_color_discrete(name="") +
  theme_classic() +
  theme(legend.position = c(0.99,0.01),legend.justification = c(1,0))+
  ggtitle(expression(paste("G-formula estimates of cumulative incidence")))
```

# Bootstrapped variance

In order to estimate confidence intervals, we will need to get the sample variance using bootstrapping. To do so, we first need to create a function that will perform the bootstrapping. Then we will use the `boot()` function from the `boot` package to perform the bootstrapping $R$ times, based on that function.

The function `gformula_effectestimates()` takes the following parameters:

- @param `baseline_data` Baseline data set at `intime=0` (created above)
- @param `index` If `NULL`, then original data will be used. If not null, then bootstrapped samples fro the baseline will be taken using the vector `index`. The default is `NULL`.
- @param `fdata` Functional data if `index` is not NULL. The default is the full `miners` data set
- @param `seed` Seed value we input so that the results are reproducible. 

```{r, class.source = 'fold-show'}
# now we turn the g-formula algorithm to get risk difference into a function in order to get bootstrap variance
gformula_effectestimates <- function(data=baseline_data, index=NULL, fdata=miners, endtime=10, seed=NULL){
  # for bootstrap samples, when needed: first take a random sample of the 
  #  study IDs, with replacement
  # 0) preprocessing: take bootstrap sample if bootstrapping, otherwise use the observed data
  if(is.null(index)){
    # if index is NULL, then just use original data and do nothing here
  } else{
    # bootstrap sample of the baseline data using the vector 'index'
    data = slice(data, index)
    # resample from the full data according to the id variable in the baseline data
    idindex = tibble(reps = table(data$id), id = as.numeric(names(table(data$id))))
    fdata = merge(idindex, fdata, by='id')
    fdata = slice(fdata, rep(1:dim(fdata)[1], times=fdata$reps))
  }
  # 1) fit models to full data/bootstrap sample of the full data
  mod_e = glm(lograd ~ outtime + cum_rad_lag1 + smoker, data=filter(data, atwork==1))
  # pooled logistic model for leaving work (at the end of the year), while at work
  mod_w = glm(leavework ~ outtime + I(outtime^2) + rad_lag1 + cum_rad_lag1 + smoker, 
              data=filter(fdata, atwork==1 | leavework==1), family=binomial(link=logit))
  # pooled logistic model for death
  mod_d = glm(dead ~ outtime + atwork + cum_rad + I(cum_rad*cum_rad) + smoker + smoker*cum_rad, 
              data=fdata, family=binomial(link=logit))
  
  # 2) take large MC sample from baseline data
  mc_iter = 20000
  set.seed(seed)
  mcdata <- slice(data, sample(1:N, size = mc_iter, replace=TRUE))
  # 3) simulate probability distributions in large MC sample using model fits from step 1
  nc = gformula_risks(intervention=NULL,        
                      pseudo_cohort_bl=mcdata, 
                      endtime=endtime, 
                      mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                      seed=seed)
  unex = gformula_risks(intervention=0,         
                        pseudo_cohort_bl=mcdata, 
                        endtime=endtime, 
                        mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                        seed=seed)
  exp3 = gformula_risks(intervention='rad>0.3', 
                        pseudo_cohort_bl=mcdata, 
                        endtime=endtime, 
                        mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                        seed=seed)
  exp1 = gformula_risks(intervention='rad>0.1', 
                        pseudo_cohort_bl=mcdata, endtime=endtime, 
                        mod_e=mod_e, mod_w=mod_w, mod_d=mod_d, 
                        seed=seed)
  # 4) summarize data
  ci_nc = as.numeric(with(nc, tapply(cum_incidence, outtime, mean)))
  ci_unex = as.numeric(with(unex, tapply(cum_incidence, outtime, mean)))
  ci_exp3 = as.numeric(with(exp3, tapply(cum_incidence, outtime, mean)))
  ci_exp1 = as.numeric(with(exp1, tapply(cum_incidence, outtime, mean)))
  # return a vector of risk differences and log-risk ratios at time t=endtime
  # could also calculate for all times
  c(
    rd_exp3_10=c(ci_exp3-ci_nc)[endtime],
    rd_exp1_10=c(ci_exp1-ci_nc)[endtime],
    rd_unex_10=c(ci_unex-ci_nc)[endtime],
    lrr_exp3_10=log(c(ci_exp3/ci_nc)[endtime]),
    lrr_exp1_10=log(c(ci_exp1/ci_nc)[endtime]),
    lrr_unex_10=log(c(ci_unex/ci_nc)[endtime])
  )
}
```






Prior to performing the bootstrapping, we will use the function to get point estimates for the risk difference at `t=10`:

```{r, class.source = 'fold-show'}
gf_est <- gformula_effectestimates(data=baseline_data, fdata=miners, endtime=10, seed=NULL)
knitr::kable(gf_est, caption="1 Bootstrapped Resample: Risk Differences")

```



In order to get confidence intervals, we will use the non-parametric bootstrap. To note: to get valid standard errors, we should use at least 200 bootstrap samples, and at least 1000 for percentile-based confidence intervals. In the interest of computation time, we will only perform 10 bootstrap samples here.

```{r}
nbootsamples = 10
boot_samples <- boot(data = baseline_data, fdata=miners, statistic = gformula_effectestimates, R = nbootsamples, endtime=10)
knitr::kable(boot_samples$t0, caption="10 Bootstrapped Resamples: Risk Differences")

```


Finally, we calculate the 95% bootstrapped confidence intervals:

```{r, class.source = 'fold-show'}
est = boot_samples$t0 #the risk differences
lci =  boot_samples$t0 - 1.96*apply(boot_samples$t, 2, sd)
uci =  boot_samples$t0 + 1.96*apply(boot_samples$t, 2, sd)

cbind.data.frame(RD = est, LB=lci, UB = uci) %>%
  knitr::kable(caption = "Risk Differences/Log Relative Risks and 95% Bootstrapped Confidence Intervals (R=10) Under Different Treatment Regimes at t=10 Using G-Formula")

```


# Exercise

Perform the bootstrap sampling for $R=200$ resamples. Plot your results and interpret them:


```{r}

boot_samples200 <- boot(data = baseline_data, fdata=miners, statistic = gformula_effectestimates, R = 200, endtime=10)

est200 = boot_samples200$t0 #the risk differences
lci200 =  boot_samples200$t0 - 1.96*apply(boot_samples200$t, 2, sd)
uci200 =  boot_samples200$t0 + 1.96*apply(boot_samples200$t, 2, sd)

dat <- cbind.data.frame(regime = c("Exp3","Exp1","Unex","Exp3","Exp1","Unex"),
                        type = c(rep("Risk Difference",3), rep("Log RR",3)),
                        RD = est200, LB=lci200, UB = uci200) 
knitr::kable(dat, caption = "Risk Differences/Log Relative Risks and 95% Bootstrapped Confidence Intervals (R=200) Under Different Treatment Regimes at t=10 Using G-Formula")

ggplot(data=dat) + 
  geom_point(aes(x=regime, y=RD, color=type)) +
  geom_errorbar(aes(ymin = LB, ymax=UB, x=regime, color=type))+
  facet_wrap(.~type) +
  ylab("Log RR/Risk Difference") +
  theme_bw() +
  ggtitle("G-Formula-Based Risk Differences/Log Relative Risk Predictions Under Different Treatment Regimes") +
  theme(legend.position = "bottom")

```






